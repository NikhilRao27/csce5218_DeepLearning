{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "5901dd50-9b1d-42ed-8bc4-f8fb59478844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 01:34:31--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "\rtest.dat              0%[                    ]       0  --.-KB/s               \rtest.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 01:34:32 (220 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n",
            "--2023-02-18 01:34:32--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 01:34:32 (210 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj14QN34mE2V",
        "outputId": "7f116eaf-59c6-4fd7-8153-d5c3ca72ac76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: curl.exe: command not found\n"
          ]
        }
      ],
      "source": [
        "!curl.exe --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HtoTyx7mE2V",
        "outputId": "70b3249a-7bb7-4f56-caf4-d2bb41d43843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: curl.exe: command not found\n"
          ]
        }
      ],
      "source": [
        "!curl.exe --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "190d77cc-6931-413e-eb28-7bfa952e6fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "zfcYAlRum9VD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):                                                #Defining the function name as read_data\n",
        "    f = open(file_name, 'r')                                             #Opening the file\n",
        "\n",
        "    data = []                                                            #Creating a list so that we can append the values\n",
        "    # Discard header line          \n",
        "    f.readline()                                                         #Reading the lines \n",
        "    for instance in f.readlines():                                       #For loop to read the data\n",
        "        if not re.search('\\t', instance): continue                       #Continue till the end of the table is reached\n",
        "        instance = list(map(int, instance.strip().split('\\t')))          #Mapping the values\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance                                       #Adding a bias\n",
        "        data += [instance]                                               #Addding the values to the list data\n",
        "    return data                                                          #Returning the data values"
      ],
      "metadata": {
        "id": "-gsWo3mQnFcp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the dot product function"
      ],
      "metadata": {
        "id": "khLxJTEgnLZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product(array1, array2):                                         #Defining a function named dot_product\n",
        "    #TODO: Return dot product of array 1 and array 2              \n",
        "    return sum([w * x for w, x in zip(array1, array2)])                  #Returns the product of two arrays"
      ],
      "metadata": {
        "id": "jhTzWyTtmm6h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the sigmoid"
      ],
      "metadata": {
        "id": "vAzLkSxbnSU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):                                                            #Defining a function named sigmoid\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "    denom= 1 + math.exp(-1*x)                                              #Formula for sigmoid function\n",
        "    total = 1/denom                                               \n",
        "    return total                                                           #Returning the values\n",
        "\n"
      ],
      "metadata": {
        "id": "BzYVe76dmm94"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output function"
      ],
      "metadata": {
        "id": "ufhPM9b39ILX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):                                               #Defining the function output\n",
        "    #TODO: return the output of the model  \n",
        "    x= dot_product(weight,instance)                                         #Calling the function dot_product\n",
        "    outpt = sigmoid(x)                                                      #Calling the sigmoid function\n",
        "    \n",
        "    return outpt                                                            #Returning the values"
      ],
      "metadata": {
        "id": "I4gfEEP0mvvQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):                                            #Defining the function predict\n",
        "    #TODO: return the prediction of the model\n",
        "    final_out = output(weights,instance)                                   #calling the function output\n",
        "    if final_out>=0.5:                                                     #If threshold is greater than 0.5\n",
        "      return 1                                                             #Returns 1\n",
        "    \n",
        "    return 0                                                               #else returns zero\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCqYi4ifmzlV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):                                         # Defining the function get_Accuracy\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0     #If condition is satisified then sum will be increased\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)                                     #Returns the accuracy"
      ],
      "metadata": {
        "id": "DKDs2MtIm4Zl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comment the training function"
      ],
      "metadata": {
        "id": "gF3Fn37a89Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):                           #Creating a train_perceptron function\n",
        "\n",
        "    #TODO: name this step\n",
        "    # \n",
        "    weights = [0] * (len(instances[0])-1)                             #Initially assigning all the values with zeros\n",
        "\n",
        "    for _ in range(epochs):                                           #Creating a for loop\n",
        "        for instance in instances:                                    #Creating a nested for loop\n",
        "            #TODO: name these steps\n",
        "            in_value = dot_product(weights, instance)                 #Calling the dot_product function\n",
        "            output = sigmoid(in_value)                                #Calling the sigmoid function\n",
        "            error = instance[-1] - output                             #Calculating the error\n",
        "            #TODO: name these steps\n",
        "            for i in range(0, len(weights)):                                    #Creating a for loop\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]    #Updating the weights\n",
        "\n",
        "    return weights                                                    #Returning the values"
      ],
      "metadata": {
        "id": "d-D9UjEsm67e"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f316daf8-df66-4a11-b907-e4133358c416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The purpose of sigmoid function is, it adds a nonlinearuty to the model, it is also called as an activation function. This activation function is responsible for the to pass a particular input to the outpur, and which also depends on the threshold value#"
      ],
      "metadata": {
        "id": "5Mv34q5MpAuG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cbeb66-7859-4781-be78-810a42969338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:                                                               #Creating a for loop\n",
        "  for tr_size in tr_percent:                                                      #Creating a nested loop\n",
        "    for epochs in num_epochs:                                                     #Creating another nested loop\n",
        "      size =  round(len(instances_tr)*tr_size/100)                                #Finding the size of the dataset\n",
        "      pre_instances = instances_tr[0:size]                                        \n",
        "      weights = train_perceptron(pre_instances, lr, epochs)                       #Calling the function train_perceptron                       \n",
        "      accuracy = get_accuracy(weights, instances_te)                              #Calling the accuracy function\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# - A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?"
      ],
      "metadata": {
        "id": "OXpFz8J6tPbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the above outcomes, we can clearly see that even when the training daset is 300 which is 75% of the data gives the same 80% outcome. Hence it is not mandatory to have 100% of the data for high accuracy even with fine tuned parameters with 75% of data can give high accuracy, and we can also get to a conclusion that even for the 75% of the data, the model is providing a high accuracy. We can also infer that a minimum of 50% data is require to see the significant increase in the accuracy**\n",
        "\n"
      ],
      "metadata": {
        "id": "I5PrqtTptnrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```"
      ],
      "metadata": {
        "id": "yk0LA_SMtQaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One of the reason could be low learning rate, when the learning rate is low, presumably there is a chance of vanish of gradient,until a certain point, whenever the loss function's partial derivative gets near to zero and disappears, the value of the derivative product diminishes. This issue is known as the vanishing gradient problem."
      ],
      "metadata": {
        "id": "-B2oxyzRz6_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Can you get higher accuracy with additional hyperparameters (higher than 80.0)?"
      ],
      "metadata": {
        "id": "TExilPt41GVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **By adding more activation functions, in other words adding more hidden layers we can increase the accuracy greater than 80%**"
      ],
      "metadata": {
        "id": "Zcf_8l0B1JuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?**"
      ],
      "metadata": {
        "id": "kQqSPQbx1tud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Although, Accuracy increases with number of epochs, but it may not be valid for every instances there may be a possible chances, if number of epochs increases there may be a chance of overfitting which results decrease of accuracy."
      ],
      "metadata": {
        "id": "e-ucz8kI3OOK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dERu9-_U1weU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}